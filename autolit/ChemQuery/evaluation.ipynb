{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda41d21",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "This notebook seeks to evaluate the pipeline from `main.py`. The example tested involves the drug target `CK1α`. \n",
    "\n",
    "A ground truth dataset is extracted from [molgluedb](https://www.molgluedb.com/browseDB), with the `Targets` filter set to `CK1α`. This returns 207 molecules from 18 papers (see `test-query-generation/tmp.txt`). Only 13 of these papers are accessible on PubMed Central (i.e., have a PMCID). We run our evaluations on those 13 papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import llm \n",
    "import ncbi \n",
    "import slogpkg\n",
    "\n",
    "from openai import OpenAI\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "slog = slogpkg.GoStyleLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    \"OPENAI_URL\": \"\",\n",
    "    \"OPENAI_API_KEY\": \"\",\n",
    "    \"MODEL_NAME\": \"\",\n",
    "    \"NCBI_EMAIL\": \"\",\n",
    "    \"NCBI_API_KEY\": \"\",\n",
    "}\n",
    "for var in env.keys():\n",
    "    env[var] = dotenv.get_key(\".env\", var)  # type: ignore # can be str or None\n",
    "    if not len(env[var]):\n",
    "        slog.Error(\"Missing required environment variable\", variable=var)\n",
    "        raise RuntimeError(f\"Missing environment variable: {var}\")\n",
    "\n",
    "Entrez.email = env[\"NCBI_EMAIL\"]\n",
    "Entrez.api_key = env[\"NCBI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(base_url=env[\"OPENAI_URL\"], api_key=env[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"evals/molgluedb_targets-CK1a.csv\", index_col=\"DATAID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990775a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8835c",
   "metadata": {},
   "source": [
    "## Processing to a Ground Truth\n",
    "Because the pipeline currently only searches PMC, we want to take this list of papers and molecules that target `CK1α` and find the PMCIDs of the relevant papers and the number of compounds from each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"https://doi.org/\" prefix to extract the raw DOI\n",
    "df[\"DOI\"] = df[\"SourceAddress_Website\"].apply(lambda x: x[len(\"https://doi.org/\") :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ece527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge rows if they have the same DOI.\n",
    "# \"compounds\" should be the number of rows in the original DataFrame that have the given DOI,\n",
    "# and we perform a sanity check to ensure rows with the same DOI have the same publication year\n",
    "def handle_year(group):\n",
    "    if group[\"Year\"].nunique() == 1:\n",
    "        return group[\"Year\"].iloc[0]\n",
    "    else:\n",
    "        raise RuntimeError(\"Years don't match\")\n",
    "\n",
    "\n",
    "df = (\n",
    "    df.groupby(\"DOI\")\n",
    "    .apply(\n",
    "        lambda group: pd.Series(\n",
    "            {\n",
    "                \"compounds\": len(group),  # Count of rows with same DOI\n",
    "            }\n",
    "        )\n",
    "    ).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb16c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987080a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['compounds'].sum() == 207 # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d417b0a",
   "metadata": {},
   "source": [
    "### [PubMed Central ID Converter API](https://pmc.ncbi.nlm.nih.gov/tools/id-converter-api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e504fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DOI to PMCID\n",
    "# Make the request\n",
    "try:\n",
    "    response = requests.get(\n",
    "        url=\"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\",\n",
    "        params={\n",
    "            \"tool\": \"gamma\",\n",
    "            \"email\": Entrez.email,\n",
    "            \"ids\": \",\".join(df[\"DOI\"]),\n",
    "            \"format\": \"json\",\n",
    "        },\n",
    "        timeout=10,\n",
    "    )\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"Success!\")\n",
    "        print(f\"Status: {data.get('status', 'unknown')}\")\n",
    "        print(f\"Response: {data}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcids = []\n",
    "for i, (record, id) in enumerate(zip(data.get(\"records\"), df[\"DOI\"])):\n",
    "    print(i, record)\n",
    "    if record.get(\"status\") == \"error\":\n",
    "        pmcid, doi = \" \", record.get(\"doi\")\n",
    "    else:\n",
    "        pmcid, doi = record.get(\"pmcid\"), record.get(\"doi\")\n",
    "\n",
    "    assert doi == id, f\"{doi} != {id}\"\n",
    "    pmcids.append(pmcid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ba7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PMCID'] = list(map(lambda x: x[len(\"PMC\"):], pmcids))\n",
    "df = df[df['PMCID'] != \"\"] # remove rows with no PMCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df # should have PMCID | compounds | ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02c07b",
   "metadata": {},
   "source": [
    "## Summarization Accuracy\n",
    "Given a ground truth list of ($n=13$) relevant PMCIDs, evaluate the pipeline's ability to extract the correct number of screened compounds. \n",
    "\n",
    "Binary score based on exact matching, mean absolute deviation, and average squared deviation are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(paper):\n",
    "    # Generate a summary for each paper\n",
    "    paper_summary: str = llm.generate_paper_summary(\n",
    "        slog, client, env[\"MODEL_NAME\"], paper[\"XML_Content\"]\n",
    "    )\n",
    "    slog.Info(\"Paper summary complete\")\n",
    "\n",
    "    # Use the summary + original paper to count the number of compounds screened\n",
    "    res = llm.generate_paper_compounds(\n",
    "        slog, client, env[\"MODEL_NAME\"], paper[\"XML_Content\"], paper_summary\n",
    "    )\n",
    "    n_screened = res if res else 0\n",
    "    slog.Info(\"Compounds screened complete\")\n",
    "\n",
    "    processed_paper = paper.copy()\n",
    "    processed_paper[\"Summary\"] = paper_summary\n",
    "    processed_paper[\"n\"] = str(n_screened)\n",
    "\n",
    "    return processed_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = ncbi.fetch_by_pmcid(df['PMCID']) # type: ignore # ignore the first 3 letters\n",
    "results = [process_paper(paper) for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8586c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [result['n'] for result in results] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "predictions = [int(x) for x in predictions]\n",
    "print(predictions)\n",
    "print(list(df['compounds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb683ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = 0\n",
    "total_abs_dev = 0\n",
    "total_sq_dev = 0\n",
    "for y_hat, y in zip(predictions, df['compounds']):\n",
    "    n_correct += 1 if y_hat == y else 0\n",
    "    total_abs_dev += abs(y_hat - y)\n",
    "    total_sq_dev += (y_hat - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slog.Info(\"Exact matching performance:\", correct=n_correct)\n",
    "slog.Info(\"Mean Absolute Deviation:\", mad=total_abs_dev / 13)\n",
    "slog.Info(\"Mean Squared Error:\", mse=total_sq_dev / 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af194735",
   "metadata": {},
   "source": [
    "## Coverage\n",
    "Given a drug target and related query keywords, verify that the LLM correctly extracts all of the relevant papers. Let's say the LLM's keyword searching finds $n$ papers, $x$ of which are the expected papers.\n",
    "\n",
    "### Precision: $\\frac{\\# \\text{ of expected papers found}}{\\# \\text{ papers returned}} = \\frac{x}{n}$\n",
    "\n",
    "### Recall: $\\frac{\\# \\text{ of expected papers found}}{\\# \\text{ of expected papers}} = \\frac{x}{13}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b22d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
